{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2071d97",
   "metadata": {},
   "source": [
    "<h1>Nuage de mots pour the picture of doran grey</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec547c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\utilisateur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Générer le nuage de mots\u001b[39;00m\n\u001b[0;32m     39\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cleaned_tweets)\n\u001b[1;32m---> 40\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollocations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Afficher le nuage de mots\u001b[39;00m\n\u001b[0;32m     44\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(wordcloud, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:639\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:621\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03mThe input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;124;03mself\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    620\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_text(text)\n\u001b[1;32m--> 621\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:410\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    408\u001b[0m frequencies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(frequencies\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frequencies) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe need at least 1 word to plot a word cloud, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(frequencies))\n\u001b[0;32m    412\u001b[0m frequencies \u001b[38;5;241m=\u001b[39m frequencies[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_words]\n\u001b[0;32m    414\u001b[0m \u001b[38;5;66;03m# largest entry will be 1\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Télécharger la liste de stop-words en français\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Lire le fichier CSV et stocker les tweets dans une liste\n",
    "pd.read_csv(\"the_dark_night.csv\")\n",
    "\n",
    "# Fonction pour enlever les hashtags, mentions, URL et stop-words d'un tweet\n",
    "def clean_tweet(tweet):\n",
    "  tweet = tweet.lower()  # Convertir le tweet en lower case\n",
    "  tweet = re.sub(r'#\\S+', '', tweet)  # enlever les hashtags\n",
    "  tweet = re.sub(r'@\\S+', '', tweet)  # enlever les mentions\n",
    "  tweet = re.sub(r'https?://\\S+', '', tweet)  # enlever les URL\n",
    "\n",
    "  # Enlever les stop-words\n",
    "  words = tweet.split()\n",
    "  words = [word for word in words if word not in stop_words]\n",
    "  tweet = ' '.join(words)\n",
    "\n",
    "  return tweet\n",
    "\n",
    "# Enlever les hashtags, mentions, URL et stop-words de chaque tweet\n",
    "cleaned_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "  cleaned_tweets.append(clean_tweet(tweet[0]))\n",
    "\n",
    "# Générer le nuage de mots\n",
    "text = ' '.join(cleaned_tweets)\n",
    "wordcloud = WordCloud(collocations = False, width = 1000, height = 600,\n",
    "                     background_color='white').generate(text)\n",
    "\n",
    "# Afficher le nuage de mots\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939f439",
   "metadata": {},
   "source": [
    "<h1>Les 10 bigrammes les plus tweets</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd24484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dorian gray', 394), ('picture dorian', 387), ('oscar wilde', 138), ('gray 1945', 96), ('1945 original', 89), ('original canvas', 89), ('canvas final', 89), ('final version', 89), ('version dorian', 89), ('dorian grays', 89)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def remove_special_characters_and_spaces(string):\n",
    "  no_special_characters = re.sub(r'[^\\w\\s]', '', string)\n",
    "  no_spaces = re.sub(r'\\s+', '', no_special_characters)\n",
    "  return no_spaces\n",
    "\n",
    "bigrams = []\n",
    "for tweet in cleaned_tweets:\n",
    "  words = tweet.split()\n",
    "  for i in range(len(words)-1):\n",
    "    word1 = remove_special_characters_and_spaces(words[i])\n",
    "    word2 = remove_special_characters_and_spaces(words[i+1])\n",
    "    if word1 and word2 and len(word1) > 1 and len(word2) > 1:  # vérifie si les mots ne sont pas des chaînes vides et si la longueur de chaque mot est supérieure à 1\n",
    "      bigram = word1 + ' ' + word2\n",
    "      bigrams.append(bigram)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# compter les bigrammes\n",
    "bigram_counts = Counter(bigrams)\n",
    "\n",
    "# afficher les bigrammes les plus fréquents\n",
    "top_bigrams = bigram_counts.most_common(10)\n",
    "print(top_bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcba2aa6",
   "metadata": {},
   "source": [
    "<h1>Les 10 trigrammes les plus tweets</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1862ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('picture dorian gray', 387), ('dorian gray 1945', 96), ('gray 1945 original', 89), ('1945 original canvas', 89), ('original canvas final', 89), ('canvas final version', 89), ('final version dorian', 89), ('version dorian grays', 89), ('dorian grays portrait', 89), ('dorian gray ugly', 60)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters_and_spaces(string):\n",
    "  no_special_characters = re.sub(r'[^\\w\\s]', '', string)\n",
    "  no_spaces = re.sub(r'\\s+', '', no_special_characters)\n",
    "  return no_spaces\n",
    "\n",
    "trigrams = []\n",
    "for tweet in cleaned_tweets:\n",
    "  words = tweet.split()\n",
    "  for i in range(len(words)-2):\n",
    "    word1 = remove_special_characters_and_spaces(words[i])\n",
    "    word2 = remove_special_characters_and_spaces(words[i+1])\n",
    "    word3 = remove_special_characters_and_spaces(words[i+2])\n",
    "    if word1 and word2 and word3 and len(word1) > 1 and len(word2) > 1 and len(word3) > 1:  # vérifie si les mots ne sont pas des chaînes vides et si la longueur de chaque mot est supérieure à 1\n",
    "      trigram = word1 + ' ' + word2 + ' ' + word3\n",
    "      trigrams.append(trigram)\n",
    "\n",
    "# compter les trigrammes\n",
    "trigram_counts = Counter(trigrams)\n",
    "\n",
    "# afficher les trigrammes les plus fréquents\n",
    "top_trigrams = trigram_counts.most_common(10)\n",
    "print(top_trigrams)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea80e7",
   "metadata": {},
   "source": [
    "<h1>Analyse de sentiments</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5492be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 321\n",
      "Negative: 104\n",
      "Neutral: 983\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "# Créer un DataFrame vide avec une colonne \"tweet\" et une colonne \"sentiment\"\n",
    "df = pd.DataFrame(columns=[\"tweet\", \"sentiment\"])\n",
    "\n",
    "# Pour chaque tweet nettoyé, calculer le score de sentiment et l'ajouter au DataFrame\n",
    "for tweet in cleaned_tweets:\n",
    "  analysis = TextBlob(tweet)\n",
    "  score = analysis.sentiment.polarity\n",
    "  df = pd.concat([df, pd.DataFrame({\"tweet\": tweet, \"sentiment\": score}, index=[0])], ignore_index=True)\n",
    "\n",
    "# Comptez le nombre de tweets positifs, négatifs et neutres\n",
    "positive = df[df[\"sentiment\"] > 0].count()[\"sentiment\"]\n",
    "negative = df[df[\"sentiment\"] < 0].count()[\"sentiment\"]\n",
    "neutral = df[df[\"sentiment\"] == 0].count()[\"sentiment\"]\n",
    "\n",
    "# Affichez les résultats\n",
    "print(\"Positive:\", positive)\n",
    "print(\"Negative:\", negative)\n",
    "print(\"Neutral:\", neutral)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7aa03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd431a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
